<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>优化器与损失函数 | 廾匸</title><meta name="author" content="Dummy"><meta name="copyright" content="Dummy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="epoch和batch_size的选择 epoch的作用在于逐步提高模型的精度，直到达到一个相对稳定的水平。 batch size过小会导致模型训练过程不稳定，容易受到噪声数据干扰；而batch size过大则会导致模型训练时间过长。但batch size应该在设备允许的条件下尽可能的大，因为数量越多一个样本分布越接近真实值。  优化器optim Adam和SGD通常是首选。Adam由于其自适应学">
<meta property="og:type" content="article">
<meta property="og:title" content="优化器与损失函数">
<meta property="og:url" content="https://dummyv07.github.io/2024/06/19/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/index.html">
<meta property="og:site_name" content="廾匸">
<meta property="og:description" content="epoch和batch_size的选择 epoch的作用在于逐步提高模型的精度，直到达到一个相对稳定的水平。 batch size过小会导致模型训练过程不稳定，容易受到噪声数据干扰；而batch size过大则会导致模型训练时间过长。但batch size应该在设备允许的条件下尽可能的大，因为数量越多一个样本分布越接近真实值。  优化器optim Adam和SGD通常是首选。Adam由于其自适应学">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dummyv07.github.io/img/eyes.jpg">
<meta property="article:published_time" content="2024-06-19T02:13:28.000Z">
<meta property="article:modified_time" content="2024-07-18T09:21:52.287Z">
<meta property="article:author" content="Dummy">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dummyv07.github.io/img/eyes.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "优化器与损失函数",
  "url": "https://dummyv07.github.io/2024/06/19/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/",
  "image": "https://dummyv07.github.io/img/eyes.jpg",
  "datePublished": "2024-06-19T02:13:28.000Z",
  "dateModified": "2024-07-18T09:21:52.287Z",
  "author": [
    {
      "@type": "Person",
      "name": "Dummy",
      "url": "https://dummyv07.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dummyv07.github.io/2024/06/19/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '优化器与损失函数',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/eyes.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/bg2.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">廾匸</span></a><a class="nav-page-title" href="/"><span class="site-name">优化器与损失函数</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">优化器与损失函数</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-19T02:13:28.000Z" title="发表于 2024-06-19 10:13:28">2024-06-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-18T09:21:52.287Z" title="更新于 2024-07-18 17:21:52">2024-07-18</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="epoch和batch-size的选择"><a href="#epoch和batch-size的选择" class="headerlink" title="epoch和batch_size的选择"></a>epoch和batch_size的选择</h3><ul>
<li>epoch的作用在于逐步提高模型的精度，直到达到一个相对稳定的水平。</li>
<li>batch size过小会导致模型训练过程不稳定，容易受到噪声数据干扰；而batch size过大则会导致模型训练时间过长。但batch size应该在设备允许的条件下尽可能的大，因为数量越多一个样本分布越接近真实值。</li>
</ul>
<h3 id="优化器optim"><a href="#优化器optim" class="headerlink" title="优化器optim"></a>优化器optim</h3><hr>
<p>Adam和SGD通常是首选。Adam由于其自适应学习率特征，通常可以快速收敛，特别是在训练初期</p>
<p>SGD可能需要更细致的调优，但其优势在于训练后期可能会达到更好的性能。</p>
<p><img src="/../images/image2.png" alt="alt text"><br><a target="_blank" rel="noopener" href="https://s2.51cto.com/images/blog/202207/15173404_62d1348cba7438155.gif">gif路径</a></p>
<p><a target="_blank" rel="noopener" href="https://ultrafisher.github.io/2020/08/23/%E5%9C%A8Hexo%E4%B8%AD%E7%94%A8Markdown%E6%B7%BB%E5%8A%A0%E7%BD%91%E7%BB%9C%E6%88%96%E6%9C%AC%E5%9C%B0%E8%A7%86%E9%A2%91%E4%BB%A5%E5%8F%8AGIF%E5%8A%A8%E5%9B%BE/">待解决问题</a></p>
<h4 id="在解决cv问题时如何选择合适的优化器"><a href="#在解决cv问题时如何选择合适的优化器" class="headerlink" title="在解决cv问题时如何选择合适的优化器"></a>在解决cv问题时如何选择合适的优化器</h4><ul>
<li><p>小规模数据集和简单网络</p>
<p>A大grad，RMSprop也可能表现良好，这些优化器对每个参数的学习率进行自适应调整，有可能有助于更快的收敛</p>
</li>
<li><p>模型泛化</p>
<p>如果你关注模型的泛化能力，可能会考虑使用AdamW，他结合了Adam的自适应学习率和权重衰减，有助于防止过拟合</p>
</li>
<li><p>资源和效率</p>
<p>如果你的计算资源有限，或者需要快速迭代，可能会选择计算成本较低的优化器，如SGD和Adagrad</p>
</li>
<li><p>研究和比赛(不用在实践中)</p>
<p>在研究和比赛中，最佳实践是尝试多种优化器，并使用交叉验证来找到最佳配置。有时候，结合不同优化器的优点，如使用SGD进行预热，然后切换到Adam，也能带来提升</p>
<p>参考文献：<a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/gradient-descent-algorithm/sgd#sui-ji-ti-du-xia-jiang">随机梯度下降</a></p>
</li>
</ul>
<hr>
<h4 id="三类梯度下降算法概述"><a href="#三类梯度下降算法概述" class="headerlink" title="三类梯度下降算法概述"></a><a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/gradient-descent-algorithm/sgd#sui-ji-ti-du-xia-jiang">三类梯度下降算法概述</a></h4><ul>
<li><p>GD(Gradient Descent)：就是没有利用Batch Size，用基于整个数据库得到梯度，梯度准确，但数据量大时，计算非常耗时，同时神经网络常是非凸的，网络最终可能收敛到初始点附近的局部最优点。</p>
</li>
<li><p>SGD(Stochastic Gradient Descent)：就是Batch Size &#x3D; 1，每次计算一个样本，梯度不准确，所以学习率要降低。</p>
</li>
<li><p>mini-batch SGD：就是选择合适的Batch size算法，mini-batch利用噪声梯度，一定程度 上缓解了GD算法直接掉进初始点附近的局部最优解。同时梯度准确了，学习率要加大。</p>
<p>凸：</p>
<ul>
<li>指的是顺着梯度方向走到底就 <strong>一定是 最优解</strong> 。</li>
<li><strong>大部分 传统机器学习 问题</strong> 都是凸的。</li>
</ul>
<p>非凸：</p>
<ul>
<li>指的是顺着梯度方向走到底只能保证是局部最优，<strong>不能保证</strong> 是全局最优。</li>
<li><strong>深度学习</strong>以及小部分传统机器学习问题都是非凸的。</li>
</ul>
</li>
</ul>
<p>最优化问题在机器学习中有非常重要的地位，很多机器学习算法都归结为求解最优化问题。在各种最优化算法中，梯度下降法是最简单、最常见的一种。</p>
<p><img src="/../images/image1.png" alt="alt text"></p>
<h3 id="学习率衰减策略"><a href="#学习率衰减策略" class="headerlink" title="学习率衰减策略"></a>学习率衰减策略</h3><hr>
<ul>
<li>固定步长衰减</li>
<li>多步长衰减</li>
<li>指数衰减</li>
<li>余弦退火</li>
<li>（拿代码去做实验）</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><hr>
<p>网络训练时常用Loss为交叉熵(衡量两个概率分布之间的距离 概率分布 – 特征分布)</p>
<p>与mse的区别 sigmoid softmax</p>
<p>nn.crossentropyloss():多分类交叉熵 等效 torch.nn.logsoftmax()</p>
<p>nn.NLLLoss():多分类交叉熵</p>
<p>nn.BCELoss():二分类交叉熵  ~ nn.sigmoid() 输入在 0~1</p>
<p><strong>log：简化计算 乘法变加法 提供稳定性 保证结果为正</strong></p>
<p>nn.BCEWithLogLoss()：this loss combines ‘sigmoid’ layer and the ‘BCELoss’ in one single</p>
<h5 id="centerloss"><a href="#centerloss" class="headerlink" title="centerloss"></a>centerloss</h5><ul>
<li>减小类内距</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#中心点暂时无法确定，我们将中心点当成网络参数，让网络自己学习</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenterLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,cls_num,feature_num</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.cls_num = cls_num</span><br><span class="line">        self.center = nn.Parameter(torch.randn(cls_num,feature_num))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,xs,ys</span>):</span><br><span class="line">        center_exp = self.center.index_select(dim=<span class="number">0</span>, index=ys.long())</span><br><span class="line">        count = torch.histc(ys, bins=self.cls_num, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=self.cls_num-<span class="number">1</span>)</span><br><span class="line">        count_exp = count.index_select(dim=<span class="number">0</span>, index=ys.long())</span><br><span class="line">        center_loss = torch.<span class="built_in">sum</span>(torch.div(torch.sqrt(torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(xs - center_exp, <span class="number">2</span>), dim=<span class="number">1</span>)), count_exp))</span><br><span class="line">        <span class="keyword">return</span> center_loss</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ArcSoftmax</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,feature_dim,cls_dim=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.W = nn.Parameter(torch.randn(feature_dim,cls_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,feature,m=<span class="number">0.5</span>,s=<span class="number">10</span></span>):</span><br><span class="line">        x = F.normalize(feature,dim=<span class="number">1</span>)</span><br><span class="line">        w = F.normalize(self.W,dim=<span class="number">0</span>)</span><br><span class="line">        cos = torch.matmul(x,w)/s</span><br><span class="line">        a = torch.acos(cos)</span><br><span class="line">        top = torch.exp(s*torch.cos(a+m))</span><br><span class="line">        down2 = torch.<span class="built_in">sum</span>(torch.exp(s*torch.cos(a)),dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)-torch.exp(s*torch.cos(a))</span><br><span class="line">        out = torch.log(top/(top+down2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MainNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>*<span class="number">1</span>,<span class="number">512</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">512</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>,<span class="number">256</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>,<span class="number">128</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">128</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>,<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line">        self.output_layer = ArcSoftmax(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">        self.center_loss_layer = CenterLoss(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">        self.nllloss = nn.NLLLoss()</span><br><span class="line">        <span class="comment"># self.crossEntropyLoss = nn.CrossEntropyLoss()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,xs</span>):</span><br><span class="line">        features = self.hidden_layer(xs)</span><br><span class="line">        outputs = self.output_layer(features)</span><br><span class="line">        <span class="keyword">return</span> features,outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLoss</span>(<span class="params">self,outputs,features,labels</span>):</span><br><span class="line">        loss_cls = self.nllloss(outputs, labels)</span><br><span class="line">        loss_center = self.center_loss_layer(features,labels)</span><br><span class="line"></span><br><span class="line">        loss = loss_cls + loss_center</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net</span>):</span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line">    opt = torch.optim.Adam(net.parameters())</span><br><span class="line">    opt = torch.optim.SGD(net.parameters())</span><br><span class="line">    EPOCH = <span class="number">500</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">        feature_loader = []</span><br><span class="line">        labels_loader = []</span><br><span class="line">        <span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            img = img.reshape(-<span class="number">1</span>,<span class="number">28</span>*<span class="number">28</span>).to(DEVICE)</span><br><span class="line">            <span class="comment"># label_ = one_hot(label,10).to(DEVICE).float()</span></span><br><span class="line">            label_ = label.to(DEVICE)</span><br><span class="line"></span><br><span class="line">            feature,cls_out = net(img)</span><br><span class="line"></span><br><span class="line">            loss = net.getLoss(cls_out,feature,label_)</span><br><span class="line"></span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line"></span><br><span class="line">            feature_loader.append(feature)</span><br><span class="line">            labels_loader.append(label)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch:<span class="subst">&#123;epoch&#125;</span>,loss:<span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">        features = torch.cat(feature_loader,<span class="number">0</span>)</span><br><span class="line">        labels = torch.cat(labels_loader)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">10</span> ==<span class="number">0</span>:</span><br><span class="line">            visualize(features.data.cpu().numpy(),labels.data.cpu().numpy(),epoch=epoch+<span class="number">100</span>)</span><br><span class="line">net = MainNet().to(DEVICE)</span><br><span class="line">train(net)</span><br></pre></td></tr></table></figure>



<h5 id="arcsoftmax"><a href="#arcsoftmax" class="headerlink" title="arcsoftmax"></a>arcsoftmax</h5><ul>
<li>增大类间距</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ArcSoftmax</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,feature_dim,cls_dim=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.W = nn.Parameter(torch.randn(feature_dim,cls_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,feature,m=<span class="number">0.5</span>,s=<span class="number">10</span></span>):</span><br><span class="line">        x = F.normalize(feature,dim=<span class="number">1</span>)</span><br><span class="line">        w = F.normalize(self.W,dim=<span class="number">0</span>)</span><br><span class="line">        cos = torch.matmul(x,w)/s</span><br><span class="line">        a = torch.acos(cos)</span><br><span class="line">        top = torch.exp(s*torch.cos(a+m))</span><br><span class="line">        down2 = torch.<span class="built_in">sum</span>(torch.exp(s*torch.cos(a)),dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)-torch.exp(s*torch.cos(a))</span><br><span class="line">        out = torch.log(top/(top+down2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MainNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MainNet, self).__init__()</span><br><span class="line">        self.conv_layers = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc_layers = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">256</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">128</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">128</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">64</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">32</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">32</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.output_layer = ArcSoftmax(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">        self.nllloss = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv_layers(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        features = self.fc_layers(x)</span><br><span class="line">        outputs = self.output_layer(features)</span><br><span class="line">        <span class="keyword">return</span> features, outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLoss</span>(<span class="params">self, outputs, labels</span>):</span><br><span class="line">        loss = self.nllloss(outputs, labels)</span><br><span class="line">        <span class="keyword">return</span> loss        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, self).__init__()</span><br><span class="line">        self.input_layer = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">64</span>,<span class="number">5</span>,bias=<span class="literal">False</span>),   <span class="comment"># 64 ,24 * 24</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">5</span>,bias=<span class="literal">False</span>), <span class="comment"># 128,20,20</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>,<span class="number">64</span>,<span class="number">5</span>,bias=<span class="literal">False</span>), <span class="comment">#  64 16,16</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">32</span>,<span class="number">5</span>,bias=<span class="literal">False</span>),  <span class="comment">#  32,12,12</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            </span><br><span class="line">        )</span><br><span class="line">        self.hidden_layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">32</span>*<span class="number">12</span>*<span class="number">12</span>,<span class="number">12</span>*<span class="number">12</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">12</span>*<span class="number">12</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>*<span class="number">12</span>,<span class="number">6</span>*<span class="number">6</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">36</span>),</span><br><span class="line">            nn.Mish(),</span><br><span class="line">            nn.Linear(<span class="number">36</span>,<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.ArcSoftmax_layer = ArcSoftmax(<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line">        self.nllLoss = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = self.input_layer(<span class="built_in">input</span>)</span><br><span class="line">        feature = self.hidden_layer(x.view(-<span class="number">1</span>,<span class="number">32</span>*<span class="number">12</span>*<span class="number">12</span>))</span><br><span class="line">        output = self.ArcSoftmax_layer(feature.view(-<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> feature,output</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLoss</span>(<span class="params">self,output,labels</span>):</span><br><span class="line">        loss = self.nllLoss(output,labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = MainNet().to(DEVICE)</span><br><span class="line"><span class="comment"># net.eval()</span></span><br><span class="line"><span class="comment"># input = torch.randn(1,1,28,28).to(DEVICE)</span></span><br><span class="line"><span class="comment"># net(input)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>特诊空间分类越好，分类效果越好</strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://dummyv07.github.io">Dummy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://dummyv07.github.io/2024/06/19/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">https://dummyv07.github.io/2024/06/19/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://dummyv07.github.io" target="_blank">廾匸</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/img/eyes.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/05/26/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a><a class="pagination-related" href="/2024/07/04/Hexo-Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84blog/" title="Hexo+Github创建自己的blog"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hexo+Github创建自己的blog</div></div><div class="info-2"><div class="info-item-1"> 参考 https://blog.fiveth.cc/p/bb32/ https://butterfly.js.org/posts/21cfbf15/     基础Blog搭建1.环境搭建 node.js  git启动终端控制台 输入node -vnpm -vgit -v   2.安装hexonpm install hexo-cli -g（如果安装失败可以以管理员模型启动cmd mac是在命令行前加上sudo）然后输入你电脑的password 就会开始下载 3.构建github仓库在github中Create a new repository，也就是创建一个新的仓库，这里需要注意的是仓库名必须和用户名一致为:用户名.github.io 这也就是之后你hexo博客的网址。 4.生成ssh keys同样是打开终端输入（如果不成功同样选择在前面加上sudo）ssh-keygen -t rsa -C “邮件地址” 然后敲4次Enter 然后进入.ssh文件打开里面的id_rsa.pub,全选复制里面的代码 然后打开github 进入用户设置，找到SSH keys 新建SSH...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/07/12/%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85/" title="模型打包"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-12</div><div class="info-item-2">模型打包</div></div><div class="info-2"><div class="info-item-1">模型打包目前自己所用的框架为pytorch(python版本的torch)，需要打包好训练好的AI模型(网络结构+网络参数)，放到其他设备上运行。 LibTorch也是一个部署时候用的推理框架  c++&#x2F;java 版本 TorchScript：torch自己的打包工具 torch保存模型的方式：1 只保存模型参数(权重文件)12345678910111213# torch.save(model.state_dict(), &#x27;model.pth&#x27;)import torchfrom torchvision.models import resnet18model = resnet18()torch.save(model.state_dict(), &#x27;model.pth&#x27;) # 只保存权重###import torchimport torchvision.models as modelsnet =...</div></div></div></a><a class="pagination-related" href="/2024/07/18/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" title="模型优化 剪枝 蒸馏 量化"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-18</div><div class="info-item-2">模型优化 剪枝 蒸馏 量化</div></div><div class="info-2"><div class="info-item-1">模型优化(性能)https://pytorch.org/tutorials/beginner/profiler.html剪枝和蒸馏不可控，实际生产中一般使用量化。部署时需要根据设备，考虑模型大小与模型计算量 剪枝连接变稀疏了，计算量减少，直接使网络中的部分神经元失活(将参数置为0，不再参与运算可以加速模型的计算)，但依赖于特定算法库或硬件平台的支持。模型深层拿到的是语义特征，模型浅层拿到的是边缘特征。  结构化剪枝破坏掉原有模型的结构1prune.random_structured(conv, name=&#x27;weight&#x27;, amount=0.5,  dim=0)  非结构化剪枝不改变原有模型的结构(层数)，改变的是每一层卷积核的个数代码：https://pytorch.org/tutorials/intermediate/pruning_tutorial.html  12345import torch.nn.utils.prune as pruneconv = model.conv1prune.random_unstructured(conv,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/eyes.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Dummy</div><div class="author-info-description">山与山不见面 再见容易再见难</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/DummyV07" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dummy.v07@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Wechat:---NoOneAndYou （注明来意）</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#epoch%E5%92%8Cbatch-size%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.</span> <span class="toc-text">epoch和batch_size的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8optim"><span class="toc-number">2.</span> <span class="toc-text">优化器optim</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E8%A7%A3%E5%86%B3cv%E9%97%AE%E9%A2%98%E6%97%B6%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.1.</span> <span class="toc-text">在解决cv问题时如何选择合适的优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E7%B1%BB%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="toc-number">2.2.</span> <span class="toc-text">三类梯度下降算法概述</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%AD%96%E7%95%A5"><span class="toc-number">3.</span> <span class="toc-text">学习率衰减策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#centerloss"><span class="toc-number">4.0.1.</span> <span class="toc-text">centerloss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#arcsoftmax"><span class="toc-number">4.0.2.</span> <span class="toc-text">arcsoftmax</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/13/Docker-Python/" title="Docker部署python项目">Docker部署python项目</a><time datetime="2025-07-13T06:00:00.000Z" title="发表于 2025-07-13 14:00:00">2025-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/30/service_manager/" title="service_manager"><img src="/img/service_manager.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="service_manager"/></a><div class="content"><a class="title" href="/2025/06/30/service_manager/" title="service_manager">service_manager</a><time datetime="2025-06-30T04:00:00.000Z" title="发表于 2025-06-30 12:00:00">2025-06-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/" title="机器学习之Logistic回归"><img src="/img/ljhg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之Logistic回归"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/" title="机器学习之Logistic回归">机器学习之Logistic回归</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="机器学习之集成学习"><img src="/img/jcxx.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之集成学习"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="机器学习之集成学习">机器学习之集成学习</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM/" title="机器学习之SVM"><img src="/img/svm.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之SVM"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM/" title="机器学习之SVM">机器学习之SVM</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Dummy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.2.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>