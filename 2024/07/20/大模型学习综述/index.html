<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型学习综述 | 廾匸</title><meta name="author" content="Dummy"><meta name="copyright" content="Dummy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大模型学习之路持续更新中… 大模型简单讲就是参数量巨大的神经网络模型 1. 基础知识1.1NLP基础知识 RNN(循环神经网络)  对于语言来说顺序是十分重要的，顺序提供了一定的信息，”我爱你”和”你爱我”，就是两个不同的情况。RNN一个重视序列信息的网络，序列即前后关系。  HIDDEN STATE(隐状态)：保存信息  缺点：数据输入时间越早，在隐状态中占据的影响就越小。  应用场景：机器翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型学习综述">
<meta property="og:url" content="https://dummyv07.github.io/2024/07/20/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="廾匸">
<meta property="og:description" content="大模型学习之路持续更新中… 大模型简单讲就是参数量巨大的神经网络模型 1. 基础知识1.1NLP基础知识 RNN(循环神经网络)  对于语言来说顺序是十分重要的，顺序提供了一定的信息，”我爱你”和”你爱我”，就是两个不同的情况。RNN一个重视序列信息的网络，序列即前后关系。  HIDDEN STATE(隐状态)：保存信息  缺点：数据输入时间越早，在隐状态中占据的影响就越小。  应用场景：机器翻译">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dummyv07.github.io/img/eyes.jpg">
<meta property="article:published_time" content="2024-07-20T10:00:00.000Z">
<meta property="article:modified_time" content="2025-07-01T09:00:30.875Z">
<meta property="article:author" content="Dummy">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dummyv07.github.io/img/eyes.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大模型学习综述",
  "url": "https://dummyv07.github.io/2024/07/20/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/",
  "image": "https://dummyv07.github.io/img/eyes.jpg",
  "datePublished": "2024-07-20T10:00:00.000Z",
  "dateModified": "2025-07-01T09:00:30.875Z",
  "author": [
    {
      "@type": "Person",
      "name": "Dummy",
      "url": "https://dummyv07.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dummyv07.github.io/2024/07/20/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型学习综述',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/eyes.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/bg2.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">廾匸</span></a><a class="nav-page-title" href="/"><span class="site-name">大模型学习综述</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大模型学习综述</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-20T10:00:00.000Z" title="发表于 2024-07-20 18:00:00">2024-07-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-01T09:00:30.875Z" title="更新于 2025-07-01 17:00:30">2025-07-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>大模型学习之路<br>持续更新中…</p>
<h1 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h1><p>简单讲就是参数量巨大的神经网络模型</p>
<h1 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h1><h2 id="1-1NLP基础知识"><a href="#1-1NLP基础知识" class="headerlink" title="1.1NLP基础知识"></a>1.1NLP基础知识</h2><ul>
<li><p>RNN(循环神经网络)</p>
<ul>
<li><p>对于语言来说顺序是十分重要的，顺序提供了一定的信息，”我爱你”和”你爱我”，就是两个不同的情况。RNN一个重视序列信息的网络，序列即前后关系。</p>
</li>
<li><p>HIDDEN STATE(隐状态)：保存信息</p>
</li>
<li><p>缺点：数据输入时间越早，在隐状态中占据的影响就越小。</p>
</li>
<li><p>应用场景：机器翻译｜图生文｜语音识别｜量化交易模型</p>
</li>
</ul>
</li>
<li><p>seq2seq</p>
<ul>
<li><p>受限于网络结构，RNN只实现N to N(机器翻译任务，一个词翻译一个词)或者1 to N，N to 1，不能解决N to M的问题</p>
</li>
<li><p>seq2seq：有Encoder和Decoder的模型，这两部分依然是RNN网络</p>
<p>Encoder提取原始信息中的意义再由Decoder将意义转换成对应的目标语言，但意义单元能够存储的信息有限，句子太长会导致精度下降，(这个问题可以通过加入attention来解决，但这样做会的计算方式太慢啦，于是就有了<a href="https://dummyv07.github.io/2024/08/21/transform/">transfomer</a>)</p>
</li>
</ul>
</li>
<li><p>LSTM(长短时记忆网络)</p>
<ul>
<li>RNN有一定的记忆能力，但它只能保留短期记忆，在各类任务上表现并不好。此时参考人类”有取舍”但记忆机制，出现了LSTM。</li>
<li>GATE(门控单元)<ul>
<li>遗忘门决定了小盒子要保留多少原有信息</li>
<li>输入门决定了当前网络信息有多少信息要被保留</li>
<li>输出门决定了多大程度的输出小盒子的星系</li>
</ul>
</li>
</ul>
</li>
<li><p>GRU(门控循环单元) LSTM的变体</p>
<ul>
<li>将输入门与遗忘门合并为更新门<ul>
<li>更新门决定丢弃哪些旧信息，更新哪些新信息</li>
<li>重置门决定写入多少上一时刻的网络状态，用来捕捉短期记忆</li>
</ul>
</li>
<li>结构简单，计算高效，效果与LSTM不相上下</li>
</ul>
</li>
<li><p>经典NLP任务</p>
<ul>
<li>情感分析(sentiment-analysis)：对给定的文本分析其情感极性</li>
<li>文本生成(text-generation)：根据给定的文本进行生成命名</li>
<li>实体识别(ner)：标记句子中的实体</li>
<li>阅读理解(question-answering)：给定上下文与问题，从上下文中抽取答案</li>
<li>掩码填充(fill-mask)：填充给定文本中的掩码词</li>
<li>文本摘要(summarization)：生成一段长文本的摘要</li>
<li>机器翻译(translation)：将文本翻译成另一种语言</li>
<li>特征提取(feature-extraction)：生成给定文本的张量表示</li>
<li>对话机器人(conversioinal)：根据用户输入文本，产生回应，与用户对话</li>
</ul>
</li>
</ul>
<h2 id="1-2LLM资源以及效率估算"><a href="#1-2LLM资源以及效率估算" class="headerlink" title="1.2LLM资源以及效率估算"></a>1.2LLM资源以及效率估算</h2><p>论文参考：LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs</p>
<p>大模型微调和推理所需的内存空间（主要指 GPU 显存）取决于多个因素，包括模型的大小、参数量、数据集大小、训练&#x2F;推理的批次大小（batch size）、精度类型（如 FP32、FP16、INT8），以及优化策略（如混合精度训练、剪枝、量化等）</p>
<ul>
<li><h4 id="LLM微调"><a href="#LLM微调" class="headerlink" title="LLM微调"></a><strong>LLM微调</strong></h4></li>
</ul>
<p>大模型微调需要大量显存来存储模型的权重、梯度、中间激活值，以及优化器状态等。影响微调内存需求的关键因素包括模型大小、批次大小和优化器选择。</p>
<ul>
<li><p><strong>VRAM</strong></p>
<ul>
<li>结论：全量微调在推理的4倍左右，推理基本就在modelweight的4-5倍左右(fp32)</li>
<li>模型参数量：7b&#x3D;70亿个参数，一个参数在FP32:占4个Byte(字节) FP16:占2个Byte INT8:占1个Byte。一个字节为8bit(位)，1kb&#x3D;1024Byte</li>
<li>微调时一般会选择FP16，此时模型大小为7B*2&#x3D;14GB</li>
<li>选用LORA微调为例：LoRA参数 &#x3D;  r * 原始权重矩阵尺寸(r&#x3D;4 通常会选择较小的秩以减少计算)</li>
<li>显存的其他开销：存储激活值、优化器状态等。</li>
</ul>
</li>
<li><h4 id="LLM-推理的核心指标以基础知识"><a href="#LLM-推理的核心指标以基础知识" class="headerlink" title="LLM 推理的核心指标以基础知识"></a>LLM 推理的核心指标以基础知识</h4><ul>
<li><p><strong>TTFT</strong>(Time To Firset Token)：首个Token延迟，即用户输入到输出第一个token之间的延迟，流式输出时，TTFT是很重要的评价指标，决定用户体验</p>
</li>
<li><p><strong>TPOT</strong>(Time Per Output Token)：每个输出token之间的延迟(不包含首个Token)。决定整个推理过程的时间</p>
</li>
<li><p><strong>Latency</strong>：延迟。从输入到输出最后一个token的延迟，Latency &#x3D; (TTFT) + (TPOT) * (the number of tokens to be generated)。</p>
<p>Latency 可以转换为 Tokens Per Second (<strong>TPS</strong>)：TPS &#x3D; (the number of tokens to be generated) &#x2F; Latency。</p>
</li>
<li><p><strong>Throughput</strong>：吞吐量，即每秒针对所有请求生成的 token 数。以上三个指标都针对单个请求，而吞吐量是针对所有并发请求的。</p>
</li>
<li><p><strong>VRAM</strong>：推理部分的显存主要用于存储 模型参数和kv缓存</p>
<ul>
<li>kv缓存是输入上下文和所有先前令牌的键&#x2F;值嵌入。它们被计算一次病存储在内存中，以便在解码阶段重用</li>
<li>显存使用影响最大批量大小和序列长度，这会大幅影响LLM推理吞吐量</li>
</ul>
<p>	</p>
</li>
<li><p>推理主要分为两个部分</p>
<ul>
<li>预填充(prefill)：模型处理输入上下文，计算embeding，计算并存储输入序列中的词块儿的<strong>key</strong>和<strong>value</strong>向量，并生成第一个输入的词块儿</li>
<li>解码(decode)：根据上下文(输入和已经生成的token)生成下一个token</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="根据业务场景计算大模型推理所需的最小GPU显存以及推理时间"><a href="#根据业务场景计算大模型推理所需的最小GPU显存以及推理时间" class="headerlink" title="根据业务场景计算大模型推理所需的最小GPU显存以及推理时间"></a><strong>根据业务场景计算大模型推理所需的最小GPU显存以及推理时间</strong></h4><ul>
<li><p>估计推理时间就是估计推理每个阶段的总内存和计算工作，并处以GPU的处理速率。</p>
<p>s：序列长度；b：批量大小；h：隐藏纬度；L：transformer层数；N：模型参数；</p>
<p>GPU FLOPs速率：对于A100，是31.2 TFLOPS</p>
<p>GPU HBM速率：对于A00，是1.5TB&#x2F;秒</p>
</li>
</ul>
</li>
</ul>
<h2 id="1-3大模型的部署与加载"><a href="#1-3大模型的部署与加载" class="headerlink" title="1.3大模型的部署与加载"></a>1.3大模型的部署与加载</h2><p>大模型的加载主要分为通过API调用，本地化部署后直接调用</p>
<ul>
<li><p>大模型框架</p>
<p>大模型框架是指用于训练、推理和部署大型语言模型(LLMs)的软件工具和库。通常提供高效的计算资源管理、分布式训练、模型优化和推理加速等功能。</p>
<ul>
<li>常见的大模型框架<ul>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/handy-ollama/#/C2/1.%20Ollama%20%E5%9C%A8%20macOS%20%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE">ollama</a><ul>
<li>API简介，易于上手｜支持硬件加速(GPU，TPU)｜高并发性受限｜高级功能支持有限</li>
</ul>
</li>
<li>vLLM<ul>
<li>极高的推理速度｜并发速度快｜优化内存使用｜支持多种分布式计算模型</li>
</ul>
</li>
<li><strong>LightLLM</strong><ul>
<li>轻量级设计｜易于拓展｜高速性能</li>
</ul>
</li>
<li>Local AI<ul>
<li>本地部署</li>
</ul>
</li>
<li>fastllm<ul>
<li>初c++实现｜可以在安卓上直接编译｜平台推理速度都很快</li>
</ul>
</li>
<li>DeepSpeed-MIL<ul>
<li>KV缓存｜连续批处理｜动态SplitFuse｜张量并行性｜高性能cuda内核</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>模型平台 HuggingFace｜ModelScope</p>
<ul>
<li><p>API调用</p>
<ul>
<li><p>openai</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv, find_dotenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载.env文件获取API-key</span></span><br><span class="line">load_dotenv(find_dotenv())</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">  messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个心理咨询师&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你好&quot;</span>&#125;</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(completion.choices[<span class="number">0</span>].message)</span><br></pre></td></tr></table></figure>
</li>
<li><p>langchain</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用langchain进行llm的建立</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv,find_dotenv</span><br><span class="line"></span><br><span class="line">load_dotenv(find_dotenv()) <span class="comment"># 加载环境变量 find_dotenv() # 找到.env文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出dotenv获取的环境变量</span></span><br><span class="line"><span class="comment"># print(f&quot;OPENAI_API_KEY: &#123;os.environ[&#x27;OPENAI_API_KEY&#x27;]&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">chat = ChatOpenAI(openai_api_key = os.environ[<span class="string">&#x27;OPENAI_API_KEY&#x27;</span>],</span><br><span class="line">                  model=<span class="string">&#x27;gpt-3.5-turbo&#x27;</span></span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line"><span class="comment"># langchain的格式如下</span></span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> SystemMessage,HumanMessage,AIMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># message 可以理解成memory</span></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;You are a helpful assistant.&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;What is the capital of France?&quot;</span>),</span><br><span class="line">    AIMessage(content=<span class="string">&quot;Paris is the capital of France.&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;What is the capital of Germany?&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">res = chat(messages)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>部署	</p>
<p>本地部署是指将软件或应用程序安装在用户本地设备（例如个人计算机、服务器或其他硬件设备）上的过程。与云端部署不同，本地部署将软件的所有组件和资源直接安装在用户的设备上，使其能够在本地运行而无需依赖云服务。</p>
<p><strong>1、本地部署的基本原理</strong></p>
<p>本地部署的基本原理是将软件的代码、数据库以及其他必要的资源存储在用户的设备上。用户通过本地设备直接访问和操作软件，不需要依赖外部服务器或云服务。这种方式使得用户能够在本地环境中完全掌控软件的运行和数据存储过程。</p>
<p><strong>2、本地部署与云端部署的区别</strong></p>
<p>与云端部署相比，本地部署的最大区别在于数据存储和计算资源的位置。云端部署将数据存储在云服务器上，而本地部署则将数据储存在用户的本地设备上。这一区别对于一些安全性要求较高或对数据隐私有严格要求的应用来说尤为重要。</p>
<ul>
<li><p>本地部署</p>
<ul>
<li><p>transformers</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;Give me a short introduction to large language model.&quot;</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;,</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span>,</span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>modelscope</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer, GenerationConfig</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Qwen/Qwen-1_8B-Chat&quot;</span>, revision=<span class="string">&#x27;master&#x27;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;Qwen/Qwen-1_8B-Chat&quot;</span>, revision=<span class="string">&#x27;master&#x27;</span>, device_map=<span class="string">&quot;auto&quot;</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 第一轮对话 1st dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;你好&quot;</span>, history=<span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 你好！很高兴为你提供帮助。</span></span><br></pre></td></tr></table></figure>

<p>上述部署方式都用到了AutoModelForCausalLM库，使用from_config和from_pretrained构建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  加载模型但是不加载权重</span></span><br><span class="line">config = AutoConfig.from_pretrained(<span class="string">&#x27;Qwen/Qwen-1_8B-Chat&#x27;</span>)<span class="comment">#加载</span></span><br><span class="line">model = AutoModelForCausalLM.from_config(config)</span><br><span class="line"><span class="comment"># 加载模型同时加载权重</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&#x27;Qwen/Qwen-1_8B-Chat&#x27;</span>，revision=<span class="string">&#x27;master&#x27;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># from_pretrained的参数解释</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. **pretrained_model_name_or_path** (str, optional):</span></span><br><span class="line"><span class="string">   - 指定要加载的预训练模型的名称或路径。这可以是模型的名称（例如，&#x27;bert-base-uncased&#x27;），也可以是模型的本地路径。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. **cache_dir** (str, optional):</span></span><br><span class="line"><span class="string">   - 指定用于缓存预训练模型配置文件的目录路径。如果设置为 `None`，将使用默认缓存目录。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. **force_download** (bool, optional):</span></span><br><span class="line"><span class="string">   - 如果设置为 `True`，将强制重新下载模型配置，覆盖任何现有的缓存。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4.  **resume_download** (bool, optional)：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - 这是可选参数，如果设置为 True，则在下载过程中重新开始下载，即使部分文件已经存在。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">5. **proxies** (`Dict[str, str]`, *optional*)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   - proxies（可选参数）：这是一个字典，用于指定代理服务器的设置。代理服务器允许您在访问互联网资源时通过中继服务器进行请求，这对于在受限网络环境中使用 Transformers 库来加载模型配置信息非常有用。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            proxies = &#123; &quot;http&quot;: &quot;http://your_http_proxy_url&quot;, &quot;https&quot;: &quot;https://your_https_proxy_url&quot; &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">6. **revision** (str, optional):</span></span><br><span class="line"><span class="string">   - 指定要加载的模型的 Git 版本（通过提交哈希）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">7. **return_unused_kwargs** (bool, optional, 默认值为 False):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   - 如果将此参数设置为 True，函数将返回未使用的配置参数。这对于识别和检查传递给函数的不受支持或未识别的参数很有用。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">8.  trust_remote_code (`bool`, *optional*, defaults to `False`)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">trust_remote_code=True：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">默认情况下，trust_remote_code 设置为 True。这意味着当您使用 from_pretrained() 方法加载模型配置文件时，它将下载来自 Hugging Face 模型中心或其他在线资源的配置文件。这是一个方便的默认行为，因为通常这些配置文件是由官方提供的，且是可信的。</span></span><br><span class="line"><span class="string">trust_remote_code=False：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果您将 trust_remote_code 设置为 False，则表示您不信任从远程下载的配置文件，希望加载本地的配置文件。这对于安全性或定制性要求较高的场景可能是有用的。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">在这种情况下，您需要提供一个本地文件路径，以明确指定要加载的配置文件</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">总之，trust_remote_code 参数允许您在使用 Hugging Face Transformers 库时控制是否信任从远程下载的配置文件。默认情况下，它被设置为 True，以方便加载官方提供的配置文件，但您可以将其设置为 False 并提供本地配置文件的路径，以进行更精细的控制。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>模型文件导入方式</p>
<ul>
<li><p>GGUF(GPT-Generated Unified Format) </p>
<ul>
<li>GGUF (GPT-Generated Unified Format) 是一种文件格式，用于保存经过微调的语言模型。这种格式旨在帮助用户方便地在不同的平台和环境之间共享和导入模型。它支持多种量化格式，可以有效减少模型文件的大小。它的前身是 GGML(GPT-Generated Model Language)，是专门为了机器学习而设计的 Tensor 库，目的是为了有一个单文件的格式，并且易在不同架构的 CPU 以及 GPU 上可以推理，但后续由于开发遇到了灵活性不足、相容性及难以维护的问题。</li>
</ul>
</li>
<li><p>Safetensors </p>
<ul>
<li>是一种用于存储深度学习模型权重的文件格式，它旨在解决安全性、效率和易用性方面的问题。目前这部分功能还有待社区成员开发，目前文档资源有限。如果正在导入的模型是以下架构之一，则可以通过 Modelfile 直接导入 Ollama。当然，你也可以将 safetensors 文件转换为 gguf 文件，再进行处理，转换过程可以参考第三部分。</li>
</ul>
</li>
</ul>
</li>
<li><p>云端部署</p>
</li>
</ul>
</li>
</ul>
<h1 id="2-主流大模型"><a href="#2-主流大模型" class="headerlink" title="2. 主流大模型"></a>2. 主流大模型</h1><h2 id="2-1-模型架构类型"><a href="#2-1-模型架构类型" class="headerlink" title="2.1 模型架构类型"></a>2.1 模型架构类型</h2><p>LLM中的三种重要模型：基座模型（base）、聊天模型（chat）和指令模型（instruct）。</p>
<ol>
<li><strong>基座模型（Base Model）</strong>：<ul>
<li>基座模型通常是指那些通过大量文本数据训练的基础语言模型。这些模型经过预训练后可以生成连贯的文本，并且能够完成诸如文本填充、问答、翻译等任务。基座模型通常是其他更复杂模型的基础，因为它们可以提供语言理解和生成的基础能力。</li>
<li>基座模型的例子包括BERT、GPT系列等。</li>
</ul>
</li>
<li><strong>聊天模型（Chat Model）</strong>：<ul>
<li>聊天模型是专门用于对话系统的语言模型，它们能够模拟人类对话，并且通常具备上下文理解能力。这类模型被设计成能够进行连续多轮对话，同时保持对话的一致性和连贯性。</li>
<li>例如，OpenAI的ChatGPT就是一种聊天模型，它可以用来创建聊天机器人，进行自然的人机交互。</li>
</ul>
</li>
<li><strong>指令模型（Instruct Model）</strong>：<ul>
<li>指令模型是一种特殊类型的语言模型，它被训练来理解和执行指令或任务。与聊天模型相比，指令模型更侧重于完成具体的任务，如编写代码、总结文档、提供建议等。</li>
<li>这种模型通常会接受一个明确的任务描述或者指令作为输入，然后根据这个指令生成相应的输出。例如，InstructGPT就是一种被训练来执行特定指令的模型。</li>
</ul>
</li>
</ol>
<h2 id="2-2-经典模型架构"><a href="#2-2-经典模型架构" class="headerlink" title="2.2 经典模型架构"></a>2.2 经典模型架构</h2><h3 id="2-2-1-Transformer"><a href="#2-2-1-Transformer" class="headerlink" title="2.2.1 Transformer"></a>2.2.1 Transformer</h3><ul>
<li><strong>核心思想</strong>：基于自注意力机制的编码器-解码器架构</li>
<li><strong>关键组件</strong>：<ul>
<li><strong>自注意力机制</strong>：允许模型关注输入序列中的不同位置</li>
<li><strong>多头注意力</strong>：并行计算多个注意力头</li>
<li><strong>位置编码</strong>：为序列中的每个位置添加位置信息</li>
<li><strong>前馈神经网络</strong>：处理注意力输出</li>
</ul>
</li>
<li><strong>优势</strong>：并行计算能力强，长距离依赖建模效果好</li>
</ul>
<h3 id="2-2-2-BERT-Bidirectional-Encoder-Representations-from-Transformers"><a href="#2-2-2-BERT-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="2.2.2 BERT (Bidirectional Encoder Representations from Transformers)"></a>2.2.2 BERT (Bidirectional Encoder Representations from Transformers)</h3><ul>
<li><strong>架构特点</strong>：双向编码器，使用掩码语言模型(MLM)和下一句预测(NSP)任务</li>
<li><strong>预训练任务</strong>：<ul>
<li>MLM：随机掩盖15%的token，预测被掩盖的token</li>
<li>NSP：判断两个句子是否相邻</li>
</ul>
</li>
<li><strong>应用场景</strong>：文本分类、命名实体识别、问答系统等</li>
</ul>
<h3 id="2-2-3-GPT-Generative-Pre-trained-Transformer"><a href="#2-2-3-GPT-Generative-Pre-trained-Transformer" class="headerlink" title="2.2.3 GPT (Generative Pre-trained Transformer)"></a>2.2.3 GPT (Generative Pre-trained Transformer)</h3><ul>
<li><strong>架构特点</strong>：单向解码器，使用自回归语言模型</li>
<li><strong>预训练任务</strong>：预测下一个token</li>
<li><strong>优势</strong>：生成能力强，适合文本生成任务</li>
</ul>
<h2 id="2-3-主流大模型介绍"><a href="#2-3-主流大模型介绍" class="headerlink" title="2.3 主流大模型介绍"></a>2.3 主流大模型介绍</h2><h3 id="2-3-1-GPT系列"><a href="#2-3-1-GPT系列" class="headerlink" title="2.3.1 GPT系列"></a>2.3.1 GPT系列</h3><ul>
<li><strong>GPT-1</strong> (2018): 1.17亿参数，开创了预训练语言模型时代</li>
<li><strong>GPT-2</strong> (2019): 15亿参数，展示了零样本学习能力</li>
<li><strong>GPT-3</strong> (2020): 1750亿参数，强大的少样本学习能力</li>
<li><strong>GPT-4</strong> (2023): 多模态能力，更强的推理和创造能力</li>
<li><strong>GPT-4o</strong> (2024): 实时多模态交互能力</li>
</ul>
<h3 id="2-3-2-LLaMA系列"><a href="#2-3-2-LLaMA系列" class="headerlink" title="2.3.2 LLaMA系列"></a>2.3.2 LLaMA系列</h3><ul>
<li><strong>LLaMA-1</strong> (2023): Meta开源的基础模型，7B-65B参数</li>
<li><strong>LLaMA-2</strong> (2023): 改进的安全性和对话能力</li>
<li><strong>Code Llama</strong>: 专门用于代码生成和理解的变体</li>
<li><strong>LLaMA-3</strong> (2024): 更强的推理能力和多模态支持</li>
</ul>
<h3 id="2-3-3-中文大模型"><a href="#2-3-3-中文大模型" class="headerlink" title="2.3.3 中文大模型"></a>2.3.3 中文大模型</h3><ul>
<li><p><strong>ChatGLM系列</strong> (智谱AI)</p>
<ul>
<li>ChatGLM-6B: 开源对话模型</li>
<li>ChatGLM2-6B: 改进版本，支持更长的上下文</li>
<li>ChatGLM3: 更强的推理和工具使用能力</li>
</ul>
</li>
<li><p><strong>Qwen系列</strong> (阿里云)</p>
<ul>
<li>Qwen-7B&#x2F;14B&#x2F;72B: 不同规模的基座模型</li>
<li>Qwen2.5: 最新版本，更强的多模态能力</li>
</ul>
</li>
<li><p><strong>Baichuan系列</strong> (百川智能)</p>
<ul>
<li>Baichuan-7B&#x2F;13B: 开源中文大模型</li>
<li>Baichuan2: 改进版本，更好的中文理解能力</li>
</ul>
</li>
<li><p><strong>InternLM系列</strong> (上海AI实验室)</p>
<ul>
<li>InternLM-7B&#x2F;20B: 开源中文大模型</li>
<li>InternLM2.5: 最新版本，更强的推理能力</li>
</ul>
</li>
</ul>
<h1 id="3-大模型优化"><a href="#3-大模型优化" class="headerlink" title="3. 大模型优化"></a>3. 大模型优化</h1><p>优化方法的选择</p>
<p>当一个大模型在一个具体的应用场景中表现没有达到我们的预期的时候我们通常会考虑两种优化策略</p>
<p>FT与PE，这里我们对比主流的两种方法RAG和FT</p>
<p>主要选型的依据是 <strong>成本</strong>｜ <strong>用户体验问题</strong>｜<strong>模型幻觉问题</strong></p>
<table>
<thead>
<tr>
<th align="left">特点对比</th>
<th>RAG(检索增强生成)</th>
<th>PT(微调)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">知识更新</td>
<td>RAG直接更新检索知识库，保持信息更新，模型无需频繁的重新训练，适合<strong>动态数据环境</strong></td>
<td>FT存储<strong>静态数据</strong>知识与数据，更新需要重新训练</td>
</tr>
<tr>
<td align="left">外部知识</td>
<td>RAG擅长利用外部资源，非常适合文档或其他<strong>结构化&#x2F;非结构化数据</strong></td>
<td>虽然FT可以对大模型进行微调以对<strong>齐预训练学到的外部知识</strong>，但对于频繁更改数据源来说不太实用</td>
</tr>
<tr>
<td align="left">数据处理</td>
<td>RAG对数据加工和处理的要求低</td>
<td>FT依赖<strong>高质量数据</strong>，有限的数据集可能不会产生显著性能的提升</td>
</tr>
<tr>
<td align="left">模型风格</td>
<td>RAG主要关注信息检索，擅长整合外部知识，但可能无法完成定制模型的行为或写作风格</td>
<td>FT允许根据特定的<strong>语气或术语</strong>调整大语言模型的行为、写作风格或特定领域的知识</td>
</tr>
<tr>
<td align="left">可解释性</td>
<td>RAG通常可以追溯到特定数据源的答案，从而提供更高等级的可解释性和可溯源性</td>
<td>FT微调就像黑匣子，并不清楚模型为什么会做出这种反应，<strong>可解释性较低</strong></td>
</tr>
<tr>
<td align="left">计算资源</td>
<td>RAG需要高效的<strong>检索策略</strong>和大型<strong>数据库</strong>相关技术。另外还需要保持外部数据源集成以及数据更新</td>
<td>FT需要准备和整理高质量的训练数据集，定义微调目标以及相应的计算资源</td>
</tr>
<tr>
<td align="left">延迟和实时要求</td>
<td>RAG需要进行数据检索，可能会有更高延迟</td>
<td>经过FT的大模型无需检索即可响应，<strong>延迟较低</strong></td>
</tr>
<tr>
<td align="left">减少幻觉</td>
<td>RAG本质上不太容易产生幻觉，因为每个回答都是建立在检索到的证据上</td>
<td>FT可以通过讲模型基于特定领域的训练数据来减少幻觉。但当面临不熟悉的输入时，它仍然可能会产生幻觉</td>
</tr>
<tr>
<td align="left">道德与隐私问题</td>
<td>RAG的道德和隐私问题来源于从外部数据库检索的文本</td>
<td>FT的道德和隐私问题则因为模型的训练数据存在<strong>敏感内容</strong></td>
</tr>
</tbody></table>
<h2 id="3-1-FT-Fine-tuning"><a href="#3-1-FT-Fine-tuning" class="headerlink" title="3.1 FT(Fine-tuning)"></a>3.1 FT(Fine-tuning)</h2><h3 id="3-1-1-微调模式"><a href="#3-1-1-微调模式" class="headerlink" title="3.1.1 微调模式"></a>3.1.1 微调模式</h3><ul>
<li><p><strong>全量微调（Full Fine-tuning</strong>：在全量微调中，所有模型参数都会被更新。这通常需要大量计算资源和存储空间，因为预训练模型可能包含数亿甚至数千亿个参数。</p>
</li>
<li><p><strong>局部微调（Partial Fine-tuning）</strong>：局部微调只更新模型的一部分参数，保持其他参数不变。LoRA 通过在部分层添加低秩矩阵来表示小规模的权重调整，从而避免更新所有参数。这样可以极大减少微调时的参数数量，减少存储和计算成本。</p>
</li>
<li><p><strong>增量微调（Incremental Fine-tuning）</strong>：增量微调通常指在通过新增参数的方式进行微调，新的知识存储在新的参数中。</p>
</li>
<li><p><strong>PEFT</strong>(Parameter Efficient Fine Tuning) <strong>参数高效微调</strong></p>
<ul>
<li><strong>Prefix Tuning（前缀微调）</strong>：<ul>
<li><strong>概念</strong>：前缀微调是一种在冻结的语言模型<strong>前面插入可训练”前缀” token</strong> 的方法。这些前缀 token 在输入序列之前加入，但模型的其他参数保持不变。通过调整前缀 token 来影响模型的输出，而无需修改原模型的参数。</li>
<li><strong>应用</strong>：常用于语言生成任务，如对话、文本生成等。特别适合大型预训练语言模型，因其能够保持原模型的能力，仅调整小部分参数来适应新任务。</li>
</ul>
</li>
<li><strong>P-tuning</strong>：<ul>
<li><strong>概念</strong>：P-tuning 是一种<strong>通过学习一组可训练的嵌入层（embedding）向量来优化模型的输入</strong>。在每个位置插入这些可学习的向量，以此引导模型在新任务中的表现。这些嵌入可以看作是提示（<strong>prompt</strong>）的延伸。</li>
<li>版本：<ul>
<li><strong>P-tuning v1</strong>：直接在输入序列的每个位置插入可学习的嵌入向量。</li>
<li><strong>P-tuning v2</strong>：改进了 v1，能够应用于更大规模模型并具备更强的表达能力，同时适用于更多下游任务。</li>
</ul>
</li>
<li><strong>应用</strong>：尤其适合自然语言理解任务，如分类、问答等。</li>
</ul>
</li>
<li><strong>LoRA（Low-Rank Adaptation）</strong>：<ul>
<li><strong>概念</strong>：LoRA 是通过引入低秩矩阵来对模型中的特定层（如注意力层）的权重进行调整。它冻结了模型的权重，只对低秩的矩阵进行训练，极大地减少了参数量，同时保持模型的表达能力。</li>
<li><strong>机制</strong>：LoRA 的核心思想是在现有模型的某些权重矩阵中引入一个低秩分解部分，仅训练该低秩分解的参数，从而在微调时减少大量的计算和存储需求。</li>
<li><strong>应用</strong>：LoRA 常用于大规模语言模型、图像生成等任务中，能在大幅减少微调参数的情况下，保持性能接近于完全微调。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>模型微调</p>
<p>为了优化模型在某个垂域上的效果</p>
<p>微调工具 xtunner llama-factory</p>
<p>示例：</p>
<p>(服务器下载 速率太慢 在本地下载，然后上传到服务器的 00:51开始上传文件大小是31GB)</p>
<p>在服务器上使用xtunner工具，使用xxx数据微调InternLM模型</p>
<p><code>! pip3 install --upgrad pip</code>升级pip</p>
<p><code>! pip3 install bitsan&gt;=0.39.0</code></p>
<p><code>! git clone https://github.com/InternLM/xtuner.git</code></p>
<p><code>cd xtuner</code> <code>pip3 install -e &#39;.[deepspeed]&#39;</code></p>
<p>模型：<a target="_blank" rel="noopener" href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat">InternLM2.5-7B-Chat</a></p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git-lfs # Git LFS（Git Large File Storage，大文件存储）是 Git 的一个拓展</span><br><span class="line">git lfs install</span><br><span class="line">git clone https://www.modelscope.cn/Shanghai_AI_Laboratory/internlm2_5-<span class="number">7</span>b-chat.git # 使用git clone</span><br><span class="line"># 成功使用下载文件大小应该是<span class="number">30</span>.<span class="number">95</span>GB</span><br></pre></td></tr></table></figure>

<p>在终端使用<code>du -sh internlm2_5-7b-chat</code>查看文件夹大小</p>
<p><code>git clone https://github.com/SmartFlowAI/EmoLLM.git</code> 你跟我讲这个是个数据集？</p>
<p>加国内镜像：git clone  <a target="_blank" rel="noopener" href="https://gitclone.com/github.com/SmartFlowAI/EmoLLM.git">https://gitclone.com/github.com/SmartFlowAI/EmoLLM.git</a> 但是加了国内镜像需要注意版本问题，有时候镜像可能会更新不及时</p>
<p>打开EmoLLM中的dataset 我们选用multi_turn_dataset_2.json</p>
<p>里面是一些conversation 的QA对</p>
<p>在&#x2F;xtuner&#x2F;xtuner&#x2F;configs&#x2F;中找到你的模型对应的py文件</p>
<h2 id="3-2-PE-PromptEngineering"><a href="#3-2-PE-PromptEngineering" class="headerlink" title="3.2 PE(PromptEngineering)"></a>3.2 PE(PromptEngineering)</h2><p>提示词工程</p>
<h3 id="3-2-1-RAG"><a href="#3-2-1-RAG" class="headerlink" title="3.2.1 RAG"></a>3.2.1 RAG</h3><hr>
<ul>
<li><p>RAG基本步骤</p>
<ul>
<li><p>步骤一：构建数据索引(index)</p>
<ul>
<li>加载不同来源，不同格式文档</li>
<li>将文档分割成块(chunk)</li>
<li>对切块的数据进行进行向量化并存储到向量化数据库</li>
</ul>
</li>
<li><p>步骤二：检索-增强(Retrieval-Augmented)</p>
<ul>
<li>通过向量相似度检索与问题最相关的k个文档</li>
<li>检索召回知识附加上下文填充至prompt</li>
</ul>
</li>
<li><p>步骤三：生成(generation)</p>
<ul>
<li>检索增强提升输入至LLM生成请求响应</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>RAG有四种变体，这些变体使得RAG能够适应不同的应用场景和需求，提升其整体性能。</p>
<ul>
<li>NaiveRAG：最基本的形式，直接从检索系统中获取信息，并将其与生成模型结合，通常使用最简单的检索方式</li>
<li>AdvancedRAG：引入更复杂的<strong>检索机制</strong>和<strong>生成策略</strong>，可能包括对检索结果的优化、重排序以及增强生成过程的上下文理解能力</li>
<li>ModularRAG：采用<strong>模块化设计</strong>，将检索和生成过程拆分为独立模块，允许更灵活的组合和优化，使得每个模块可以独立改进</li>
<li>GraphRAG：利用<strong>图结构</strong>表示和检索信息，通过图神经网络等方法增强信息的关联性和上下文理解，特别适合处理复杂性关系和多模态数据</li>
</ul>
<hr>
<p>框架选择：<a href="https://dummyv07.github.io/2024/08/06/langchain/">langchain</a>｜<a href="">llamaindex</a>｜<a href="">Dify</a></p>
<p>关于框架的使用：框架在一定程度上简化我们的工作流程，但另一方面会限制我们的开发自由度，在个性化功能时，需要修改的东西太多啦，重开发的成本较高。</p>
<p><a target="_blank" rel="noopener" href="https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents">为什么我们不再使用 LangChain 来构建我们的 AI 代理</a></p>
<h3 id="3-2-2-COT"><a href="#3-2-2-COT" class="headerlink" title="3.2.2 COT"></a>3.2.2 COT</h3><p>链式思考提示：这个过程需要手工制作有效切多样化的例子，这种手动工作可能会导致次优解决方案</p>
<h3 id="3-2-3-TOT"><a href="#3-2-3-TOT" class="headerlink" title="3.2.3 TOT"></a>3.2.3 TOT</h3><p>Tree of Thoughts是一种提示技术，他允许LLm思考问题，并生成一系列步骤，然后根据步骤生成答案</p>
<p>Agent</p>
<p>Agent的能力取决于LLM的意图理解，任务拆分，推理实现和能力调度能力</p>
<h1 id="4-大模型的加速"><a href="#4-大模型的加速" class="headerlink" title="4.大模型的加速"></a>4.大模型的加速</h1><h2 id="4-1算法优化"><a href="#4-1算法优化" class="headerlink" title="4.1算法优化"></a>4.1算法优化</h2><p>算法优化时大模型加速的重要手段。例如：通过引入注意力机制(Attention Mechanism)和自注意力(self-attention)等结构，可以显著提高模型的表征能力和计算效率。</p>
<p>此外还有一些针对大模型的优化算法，如混合精度训练(Mixed Precision Training)，梯度累计(Gradient Accmulation)</p>
<h2 id="4-2硬件加速"><a href="#4-2硬件加速" class="headerlink" title="4.2硬件加速"></a>4.2硬件加速</h2><p>硬件加速是另外一种重要的优化手段。随着专用加速器(TPU,GPU)的不断发展，大模型的计算速度得到了显著提升</p>
<h2 id="4-3软件工程"><a href="#4-3软件工程" class="headerlink" title="4.3软件工程"></a>4.3软件工程</h2><p>一方面，通过优化代码实现，可以提高模型的运行效率。另一方面，通过并行计算和分布式训练，可以充分利用多核cpu和多GPU多计算资源，进一步提高模型的进行训练速度</p>
<h1 id="5-大模型评估"><a href="#5-大模型评估" class="headerlink" title="5. 大模型评估"></a>5. 大模型评估</h1><h2 id="5-1-评估指标"><a href="#5-1-评估指标" class="headerlink" title="5.1 评估指标"></a>5.1 评估指标</h2><h3 id="5-1-1-语言建模指标"><a href="#5-1-1-语言建模指标" class="headerlink" title="5.1.1 语言建模指标"></a>5.1.1 语言建模指标</h3><ul>
<li>**困惑度(Perplexity)**：衡量模型对文本的预测准确性</li>
<li><strong>BLEU</strong>：机器翻译质量评估</li>
<li><strong>ROUGE</strong>：文本摘要质量评估</li>
</ul>
<h3 id="5-1-2-能力评估"><a href="#5-1-2-能力评估" class="headerlink" title="5.1.2 能力评估"></a>5.1.2 能力评估</h3><ul>
<li>**MMLU (Massive Multitask Language Understanding)**：多任务语言理解能力</li>
<li><strong>HellaSwag</strong>：常识推理能力</li>
<li><strong>TruthfulQA</strong>：事实准确性评估</li>
<li><strong>HumanEval</strong>：代码生成能力评估</li>
</ul>
<h3 id="5-1-3-安全性评估"><a href="#5-1-3-安全性评估" class="headerlink" title="5.1.3 安全性评估"></a>5.1.3 安全性评估</h3><ul>
<li><strong>Toxicity</strong>：有害内容生成倾向</li>
<li><strong>Bias</strong>：偏见检测</li>
<li><strong>Privacy</strong>：隐私保护能力</li>
</ul>
<h2 id="5-2-评估基准"><a href="#5-2-评估基准" class="headerlink" title="5.2 评估基准"></a>5.2 评估基准</h2><h3 id="5-2-1-综合评估基准"><a href="#5-2-1-综合评估基准" class="headerlink" title="5.2.1 综合评估基准"></a>5.2.1 综合评估基准</h3><ul>
<li><strong>C-Eval</strong>：中文大模型评估基准</li>
<li><strong>CMMLU</strong>：中文多任务语言理解基准</li>
<li><strong>AlpacaEval</strong>：指令跟随能力评估</li>
</ul>
<h3 id="5-2-2-专业领域评估"><a href="#5-2-2-专业领域评估" class="headerlink" title="5.2.2 专业领域评估"></a>5.2.2 专业领域评估</h3><ul>
<li><strong>CodeXGLUE</strong>：代码理解和生成评估</li>
<li><strong>MATH</strong>：数学推理能力评估</li>
<li><strong>MedQA</strong>：医学问答能力评估</li>
</ul>
<h1 id="6-大模型应用"><a href="#6-大模型应用" class="headerlink" title="6. 大模型应用"></a>6. 大模型应用</h1><h2 id="6-1-通用应用场景"><a href="#6-1-通用应用场景" class="headerlink" title="6.1 通用应用场景"></a>6.1 通用应用场景</h2><h3 id="6-1-1-对话系统"><a href="#6-1-1-对话系统" class="headerlink" title="6.1.1 对话系统"></a>6.1.1 对话系统</h3><ul>
<li><strong>客服机器人</strong>：自动回答客户问题</li>
<li><strong>教育助手</strong>：个性化学习辅导</li>
<li><strong>心理咨询</strong>：心理健康支持</li>
</ul>
<h3 id="6-1-2-内容生成"><a href="#6-1-2-内容生成" class="headerlink" title="6.1.2 内容生成"></a>6.1.2 内容生成</h3><ul>
<li><strong>文本创作</strong>：小说、诗歌、新闻等创作</li>
<li><strong>代码生成</strong>：自动编程助手</li>
<li><strong>多模态生成</strong>：图像、音频、视频生成</li>
</ul>
<h3 id="6-1-3-信息处理"><a href="#6-1-3-信息处理" class="headerlink" title="6.1.3 信息处理"></a>6.1.3 信息处理</h3><ul>
<li><strong>文档摘要</strong>：长文档自动摘要</li>
<li><strong>信息抽取</strong>：结构化信息提取</li>
<li><strong>翻译服务</strong>：多语言翻译</li>
</ul>
<h2 id="6-2-专业领域应用"><a href="#6-2-专业领域应用" class="headerlink" title="6.2 专业领域应用"></a>6.2 专业领域应用</h2><h3 id="6-2-1-金融领域"><a href="#6-2-1-金融领域" class="headerlink" title="6.2.1 金融领域"></a>6.2.1 金融领域</h3><ul>
<li><strong>风险评估</strong>：信用评估、投资分析</li>
<li><strong>合规检查</strong>：法规遵循性检查</li>
<li><strong>市场分析</strong>：趋势预测、报告生成</li>
</ul>
<h3 id="6-2-2-医疗健康"><a href="#6-2-2-医疗健康" class="headerlink" title="6.2.2 医疗健康"></a>6.2.2 医疗健康</h3><ul>
<li><strong>诊断辅助</strong>：症状分析、诊断建议</li>
<li><strong>药物研发</strong>：分子设计、临床试验分析</li>
<li><strong>健康管理</strong>：个性化健康建议</li>
</ul>
<h3 id="6-2-3-法律领域"><a href="#6-2-3-法律领域" class="headerlink" title="6.2.3 法律领域"></a>6.2.3 法律领域</h3><ul>
<li><strong>合同审查</strong>：法律文件分析</li>
<li><strong>案例检索</strong>：相似案例查找</li>
<li><strong>法律咨询</strong>：基础法律问题解答</li>
</ul>
<h1 id="7-发展趋势与挑战"><a href="#7-发展趋势与挑战" class="headerlink" title="7. 发展趋势与挑战"></a>7. 发展趋势与挑战</h1><h2 id="7-1-技术发展趋势"><a href="#7-1-技术发展趋势" class="headerlink" title="7.1 技术发展趋势"></a>7.1 技术发展趋势</h2><h3 id="7-1-1-模型规模"><a href="#7-1-1-模型规模" class="headerlink" title="7.1.1 模型规模"></a>7.1.1 模型规模</h3><ul>
<li><strong>参数规模</strong>：从千亿向万亿参数发展</li>
<li><strong>训练数据</strong>：多模态数据融合</li>
<li><strong>架构创新</strong>：更高效的注意力机制</li>
</ul>
<h3 id="7-1-2-能力提升"><a href="#7-1-2-能力提升" class="headerlink" title="7.1.2 能力提升"></a>7.1.2 能力提升</h3><ul>
<li><strong>多模态能力</strong>：文本、图像、音频、视频统一处理</li>
<li><strong>工具使用</strong>：调用外部API和工具</li>
<li><strong>推理能力</strong>：更强的逻辑推理和数学能力</li>
</ul>
<h3 id="7-1-3-效率优化"><a href="#7-1-3-效率优化" class="headerlink" title="7.1.3 效率优化"></a>7.1.3 效率优化</h3><ul>
<li><strong>训练效率</strong>：更快的训练速度和更低的资源消耗</li>
<li><strong>推理效率</strong>：更快的响应速度和更低的延迟</li>
<li><strong>部署便利性</strong>：更简单的部署和运维</li>
</ul>
<h2 id="7-2-面临的挑战"><a href="#7-2-面临的挑战" class="headerlink" title="7.2 面临的挑战"></a>7.2 面临的挑战</h2><h3 id="7-2-1-技术挑战"><a href="#7-2-1-技术挑战" class="headerlink" title="7.2.1 技术挑战"></a>7.2.1 技术挑战</h3><ul>
<li><strong>幻觉问题</strong>：生成虚假或错误信息</li>
<li><strong>偏见问题</strong>：训练数据中的偏见传递</li>
<li><strong>安全性</strong>：恶意使用和攻击防护</li>
</ul>
<h3 id="7-2-2-社会挑战"><a href="#7-2-2-社会挑战" class="headerlink" title="7.2.2 社会挑战"></a>7.2.2 社会挑战</h3><ul>
<li><strong>就业影响</strong>：自动化对就业市场的影响</li>
<li><strong>隐私保护</strong>：个人数据隐私问题</li>
<li><strong>伦理问题</strong>：AI决策的伦理考量</li>
</ul>
<h3 id="7-2-3-监管挑战"><a href="#7-2-3-监管挑战" class="headerlink" title="7.2.3 监管挑战"></a>7.2.3 监管挑战</h3><ul>
<li><strong>法律法规</strong>：AI相关法律法规的制定</li>
<li><strong>标准制定</strong>：AI安全和质量标准</li>
<li><strong>国际合作</strong>：跨国AI治理合作</li>
</ul>
<h1 id="8-学习资源与工具"><a href="#8-学习资源与工具" class="headerlink" title="8. 学习资源与工具"></a>8. 学习资源与工具</h1><h2 id="8-1-学习资源"><a href="#8-1-学习资源" class="headerlink" title="8.1 学习资源"></a>8.1 学习资源</h2><h3 id="8-1-1-书籍推荐"><a href="#8-1-1-书籍推荐" class="headerlink" title="8.1.1 书籍推荐"></a>8.1.1 书籍推荐</h3><ul>
<li>《Attention Is All You Need》- Transformer原始论文</li>
<li>《BERT: Pre-training of Deep Bidirectional Transformers》- BERT论文</li>
<li>《Language Models are Few-Shot Learners》- GPT-3论文</li>
</ul>
<h3 id="8-1-2-在线课程"><a href="#8-1-2-在线课程" class="headerlink" title="8.1.2 在线课程"></a>8.1.2 在线课程</h3><ul>
<li>Stanford CS224N: Natural Language Processing with Deep Learning</li>
<li>MIT 6.S191: Introduction to Deep Learning</li>
<li>Coursera: Natural Language Processing Specialization</li>
</ul>
<h3 id="8-1-3-实践平台"><a href="#8-1-3-实践平台" class="headerlink" title="8.1.3 实践平台"></a>8.1.3 实践平台</h3><ul>
<li><strong>Hugging Face</strong>：模型库和数据集</li>
<li><strong>Kaggle</strong>：机器学习竞赛平台</li>
<li><strong>Papers With Code</strong>：论文和代码集合</li>
</ul>
<h2 id="8-2-开发工具"><a href="#8-2-开发工具" class="headerlink" title="8.2 开发工具"></a>8.2 开发工具</h2><h3 id="8-2-1-框架和库"><a href="#8-2-1-框架和库" class="headerlink" title="8.2.1 框架和库"></a>8.2.1 框架和库</h3><ul>
<li><strong>PyTorch</strong>：深度学习框架</li>
<li><strong>Transformers</strong>：Hugging Face的Transformer库</li>
<li><strong>Accelerate</strong>：分布式训练库</li>
<li><strong>PEFT</strong>：参数高效微调库</li>
</ul>
<h3 id="8-2-2-部署工具"><a href="#8-2-2-部署工具" class="headerlink" title="8.2.2 部署工具"></a>8.2.2 部署工具</h3><ul>
<li><strong>Docker</strong>：容器化部署</li>
<li><strong>Kubernetes</strong>：容器编排</li>
<li><strong>Gradio</strong>：快速Web界面构建</li>
<li><strong>Streamlit</strong>：数据应用开发</li>
</ul>
<hr>
<p><strong>持续更新中…</strong></p>
<p><em>最后更新时间：2024年12月</em></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://dummyv07.github.io">Dummy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://dummyv07.github.io/2024/07/20/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/">https://dummyv07.github.io/2024/07/20/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://dummyv07.github.io" target="_blank">廾匸</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/eyes.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/07/18/OpenCV/" title="OpenCV"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">OpenCV</div></div><div class="info-2"><div class="info-item-1">OpenCV-图像的基础操作图像的读取、显示与保存 图片的读取与显示12345678910import cv2img = cv2.imread(&#x27;img_path&#x27;)cv2.imshow(&#x27;windowname&#x27;,img)cv2.waitkey(0) # 让窗口持续显示 不会一闪而过import matplotlib.pyplot as plt# 因为我用的vscode，更好的图片显示方式是pltplt.imshow(img) 使用PIL读取图片12345from PIL import Imageimage = Image.open(image_path)image.show()   PIL打开的通道顺序是RGB opencv打开图片的通道顺序是BGR shape: opencv(820,818,3) PIL(670760,3)   图片的保存  cv2.imwrite(‘img_name.jpg’,img)  视频的读取与显示123456789import cv2cap =...</div></div></div></a><a class="pagination-related" href="/2024/07/30/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/" title="提示词工程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">提示词工程</div></div><div class="info-2"><div class="info-item-1">LINK 提示词工程 prompt engineering什么是提示词工程：我们在使用AI时，必须提出具体要求，AI才知道如何去完成。但是我们下达的指令并不会直接进入AI模型，prompt engineering会对我们的输入进行处理转换成promprt，再输入到模型。这里的prompt可以理解一个把我们的指令进行复杂化的过程。(即赋予我们的指令更多的信息)prompt engineering帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。研究人员可利用提示工程来提升大语言模型处理复杂任务场景的能力，如问答和算术推理能力。开发人员可通过提示工程设计、研发强大的工程技术，实现和大语言模型或其他生态工具的高效接轨。 大模型参数设置 Temperature： 参数值越小，输出越确定，反之则越随机。 Top_p： 参数值越大，输出越随机，反之则越确定。 Max length: 输出文本最大长度。 stop sequence: 输出文本结束符。 Frequency penalty: 输出文本中重复词的惩罚系数。 Presence...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/10/LlamaFactory%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/" title="LlamaFactory参数详解"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-10</div><div class="info-item-2">LlamaFactory参数详解</div></div><div class="info-2"><div class="info-item-1">LlamaFactory参数详解Link LlamaFactory是一个简单易上手的大模型训练工具 微调的概念微调是指在一个已经预训练的模型基础上进行进一步的训练。预训练模型通常是在大规模数据集(如imageNet或大型文本语料库)上训练的，因此已经捕获了丰富的特征和知识。微调的目标是利用预训练模型的知识，在较小的数据集砂锅进行特定任务的优化。 主要特点1.预训练模型：基于已经训练好的模型进行 2.较少数据：通常只需要较小的数据集 3.较短时间：训练时间相对较短，因为模型已经有了良好的初始化 4.目标：适应特定任务或领域，优化模型性能 1.微调方法LORALoRA(低秩微调，Low-Rank Adaption)是一种通过低秩近似方法来减少模型参数数量和计算量的技术。它的主要目标是通过将原始的高纬参数矩阵分解成两个低秩矩阵的乘积(W ≈ A · B)，从而实现模型的参数压缩和计算加速。 其中：  W 是原始的高维参数矩阵。 A 和 B 是低秩矩阵，其秩（rank）远小于 W 的维度。 A的维度为 m x r 。 B的维度为 r x n 。 通过这种分解，我们可以将参数数量从 m...</div></div></div></a><a class="pagination-related" href="/2024/11/24/OpenAI/" title="OpenAI"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">OpenAI</div></div><div class="info-2"><div class="info-item-1">OpenAILink OpenAI最核心的功能，就是它所提供的文本生成模型。模型经过训练可以理解自然语言、代码和图像。模型可以接受任意类型的输入，最终输出文本。 使用模型，你可以构建任意你所需要的AI应用程序，比如： 1.编写文案 2.编写编程代码 3.回答知识库问题 4.分析文本 5.日常助手 6.语言翻译 01第一个聊天程序ChatCompletion的输入是一个message list，返回是一个chatCompletion对象，示例代码如下： 123456789101112131415161718192021222324#openai默认的声明方式，注册openai后在对应的控制台获取api_keyfrom openai import OpenAIclient = OpenAI(api_key=&quot;sk-&quot;)# 通常我会把自己的API-KEY放在.env文件里，然后gitignore掉。# .env文件可以通过 dotenv库来读取，然后放进系统变量里，这样OpenAI就可以直接识别from dotenv import load_dotenv,...</div></div></div></a><a class="pagination-related" href="/2024/09/22/Tokenizer/" title="Tokenizer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">Tokenizer</div></div><div class="info-2"><div class="info-item-1">Tokenizer 作用：将文本序列转化为数字(token)序列，作为transformer的输入 分词粒度：word；character；subword  Word Tokenizer按照词进行分词，如：”I love you” -&gt; [“I”, “love”, “you”]优点：简单，容易理解，便于理解模型输出结果缺点：每个word分配一个id，所需的vocabulary根据语料大小而不同，会将意思一致的词分成两个不同的id Character Tokenizer按照字符进行分词，如：”I love you” -&gt; [“I”, “ “, “l”, “o”, “v”, “e”, “ “, “y”, “o”, “u”]优点：vocabulary相对小的多，适合中文缺点：对于英语来说，分词后的每个字符是毫无意义的，且输入的长度会变长 Subword Tokenizer按照词的子词进行分词，常用于英语，如‘today is...</div></div></div></a><a class="pagination-related" href="/2025/02/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B9%8BDeepSeek/" title="大模型学习之DeepSeek"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-09</div><div class="info-item-2">大模型学习之DeepSeek</div></div><div class="info-2"><div class="info-item-1">个人学习笔记，如有错误欢迎指正 参考链接🔗： B站视频 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning 引言因为要看DeepSeek的优化的部分，绕不开的就是MoE 再然后就是COT  MoE 混合专家模型混合专家模型详解 MoE发展历史Jacobs et al 1991 每个专家都是独立的FFN，Gating是FFN，由Gating来决定输出那一个专家的结果   2017...</div></div></div></a><a class="pagination-related" href="/2024/10/04/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%9A%E6%A8%A1%E6%80%81/" title="大模型学习之多模态"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-04</div><div class="info-item-2">大模型学习之多模态</div></div><div class="info-2"><div class="info-item-1">参考 多模态简述1. 多模态学习的概念 模态(Modality)：食物表达或感知的方式 多模态(multimodal)：研究异构(heterogeneous)和相互链接数据(interconnected data)的科学 多模态的行为和信号：   2.多模态学习六大挑战及经典工作 挑战一：Representation Learning 表式学习  学习不同模态之间交叉交互，包括融合，协调和分裂等子挑战。   挑战二：Aligment 对齐  连接，对齐表示，分割，将不同模态之间的信息进行关联对齐   挑战三：Reasoning 推理  结构 中间概念，外部范式，知识建模，不仅要求理解单个模态的信息，还要要求理解不同模态之间的信息如何进行交互，影响最终推理决策   挑战四：Generation 生成  摘要，翻译和生成，创造性的理解和生成信息一致的信息   挑战五：Transference 迁移  在模态之间转换知识，通过用一个模态的知识来提高另一个模态的能力   挑战六：Quantification 量化  更好的理解异构性，交叉模态交互，以及多模态学习的过程    </div></div></div></a><a class="pagination-related" href="/2024/12/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/" title="大模型学习之大模型文件结构"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-05</div><div class="info-item-2">大模型学习之大模型文件结构</div></div><div class="info-2"><div class="info-item-1">下载途径不一样是否文件构成也不一样？ 以下内容以Xinference加载的Qwen2_5-InstructionAWQ-14B模型为例。  config.json configuration.json generation_config.json LIENSE merges.txt model.safetensors.index.json .safeatensors README.md tokenizer.json tokenizer_config.json vocab.json  1. config.json 配置文件 描述模型的架构、超参数和训练时的设置。包括模型的层数、隐藏单元数、激活函数等信息。 {“architectures”: [  “Qwen2ForCausalLM”],“attention_dropout”: 0.0,“bos_token_id”: 151643,“eos_token_id”: 151645,“hidden_act”: “silu”,“hidden_size”: 5120,“initializer_range”:...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/eyes.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Dummy</div><div class="author-info-description">山与山不见面 再见容易再见难</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/DummyV07" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dummy.v07@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Wechat:---NoOneAndYou （注明来意）</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">大模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">1. 基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">2.1.</span> <span class="toc-text">1.1NLP基础知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2LLM%E8%B5%84%E6%BA%90%E4%BB%A5%E5%8F%8A%E6%95%88%E7%8E%87%E4%BC%B0%E7%AE%97"><span class="toc-number">2.2.</span> <span class="toc-text">1.2LLM资源以及效率估算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM%E5%BE%AE%E8%B0%83"><span class="toc-number">2.2.0.1.</span> <span class="toc-text">LLM微调</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM-%E6%8E%A8%E7%90%86%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8C%87%E6%A0%87%E4%BB%A5%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">2.2.0.2.</span> <span class="toc-text">LLM 推理的核心指标以基础知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E8%AE%A1%E7%AE%97%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%89%80%E9%9C%80%E7%9A%84%E6%9C%80%E5%B0%8FGPU%E6%98%BE%E5%AD%98%E4%BB%A5%E5%8F%8A%E6%8E%A8%E7%90%86%E6%97%B6%E9%97%B4"><span class="toc-number">2.2.0.3.</span> <span class="toc-text">根据业务场景计算大模型推理所需的最小GPU显存以及推理时间</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">2.3.</span> <span class="toc-text">1.3大模型的部署与加载</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">2. 主流大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 模型架构类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 经典模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-Transformer"><span class="toc-number">3.2.1.</span> <span class="toc-text">2.2.1 Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-BERT-Bidirectional-Encoder-Representations-from-Transformers"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2.2 BERT (Bidirectional Encoder Representations from Transformers)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-GPT-Generative-Pre-trained-Transformer"><span class="toc-number">3.2.3.</span> <span class="toc-text">2.2.3 GPT (Generative Pre-trained Transformer)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 主流大模型介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-GPT%E7%B3%BB%E5%88%97"><span class="toc-number">3.3.1.</span> <span class="toc-text">2.3.1 GPT系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-LLaMA%E7%B3%BB%E5%88%97"><span class="toc-number">3.3.2.</span> <span class="toc-text">2.3.2 LLaMA系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-%E4%B8%AD%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.3.</span> <span class="toc-text">2.3.3 中文大模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">3. 大模型优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-FT-Fine-tuning"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 FT(Fine-tuning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%BC%8F"><span class="toc-number">4.1.1.</span> <span class="toc-text">3.1.1 微调模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-PE-PromptEngineering"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 PE(PromptEngineering)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-RAG"><span class="toc-number">4.2.1.</span> <span class="toc-text">3.2.1 RAG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-COT"><span class="toc-number">4.2.2.</span> <span class="toc-text">3.2.2 COT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-TOT"><span class="toc-number">4.2.3.</span> <span class="toc-text">3.2.3 TOT</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E9%80%9F"><span class="toc-number">5.</span> <span class="toc-text">4.大模型的加速</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">4.1算法优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F"><span class="toc-number">5.2.</span> <span class="toc-text">4.2硬件加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B"><span class="toc-number">5.3.</span> <span class="toc-text">4.3软件工程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">6.</span> <span class="toc-text">5. 大模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1%E6%8C%87%E6%A0%87"><span class="toc-number">6.1.1.</span> <span class="toc-text">5.1.1 语言建模指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-%E8%83%BD%E5%8A%9B%E8%AF%84%E4%BC%B0"><span class="toc-number">6.1.2.</span> <span class="toc-text">5.1.2 能力评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-3-%E5%AE%89%E5%85%A8%E6%80%A7%E8%AF%84%E4%BC%B0"><span class="toc-number">6.1.3.</span> <span class="toc-text">5.1.3 安全性评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E8%AF%84%E4%BC%B0%E5%9F%BA%E5%87%86"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 评估基准</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-%E7%BB%BC%E5%90%88%E8%AF%84%E4%BC%B0%E5%9F%BA%E5%87%86"><span class="toc-number">6.2.1.</span> <span class="toc-text">5.2.1 综合评估基准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-%E4%B8%93%E4%B8%9A%E9%A2%86%E5%9F%9F%E8%AF%84%E4%BC%B0"><span class="toc-number">6.2.2.</span> <span class="toc-text">5.2.2 专业领域评估</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">6. 大模型应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E9%80%9A%E7%94%A8%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 通用应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-1-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F"><span class="toc-number">7.1.1.</span> <span class="toc-text">6.1.1 对话系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-2-%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90"><span class="toc-number">7.1.2.</span> <span class="toc-text">6.1.2 内容生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-3-%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86"><span class="toc-number">7.1.3.</span> <span class="toc-text">6.1.3 信息处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E4%B8%93%E4%B8%9A%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 专业领域应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F"><span class="toc-number">7.2.1.</span> <span class="toc-text">6.2.1 金融领域</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-%E5%8C%BB%E7%96%97%E5%81%A5%E5%BA%B7"><span class="toc-number">7.2.2.</span> <span class="toc-text">6.2.2 医疗健康</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3-%E6%B3%95%E5%BE%8B%E9%A2%86%E5%9F%9F"><span class="toc-number">7.2.3.</span> <span class="toc-text">6.2.3 法律领域</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">8.</span> <span class="toc-text">7. 发展趋势与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF"><span class="toc-number">8.1.</span> <span class="toc-text">7.1 技术发展趋势</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-1-%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1"><span class="toc-number">8.1.1.</span> <span class="toc-text">7.1.1 模型规模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-2-%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8D%87"><span class="toc-number">8.1.2.</span> <span class="toc-text">7.1.2 能力提升</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-3-%E6%95%88%E7%8E%87%E4%BC%98%E5%8C%96"><span class="toc-number">8.1.3.</span> <span class="toc-text">7.1.3 效率优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">8.2.</span> <span class="toc-text">7.2 面临的挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-1-%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98"><span class="toc-number">8.2.1.</span> <span class="toc-text">7.2.1 技术挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-2-%E7%A4%BE%E4%BC%9A%E6%8C%91%E6%88%98"><span class="toc-number">8.2.2.</span> <span class="toc-text">7.2.2 社会挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-3-%E7%9B%91%E7%AE%A1%E6%8C%91%E6%88%98"><span class="toc-number">8.2.3.</span> <span class="toc-text">7.2.3 监管挑战</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E4%B8%8E%E5%B7%A5%E5%85%B7"><span class="toc-number">9.</span> <span class="toc-text">8. 学习资源与工具</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90"><span class="toc-number">9.1.</span> <span class="toc-text">8.1 学习资源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-1-%E4%B9%A6%E7%B1%8D%E6%8E%A8%E8%8D%90"><span class="toc-number">9.1.1.</span> <span class="toc-text">8.1.1 书籍推荐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-2-%E5%9C%A8%E7%BA%BF%E8%AF%BE%E7%A8%8B"><span class="toc-number">9.1.2.</span> <span class="toc-text">8.1.2 在线课程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-3-%E5%AE%9E%E8%B7%B5%E5%B9%B3%E5%8F%B0"><span class="toc-number">9.1.3.</span> <span class="toc-text">8.1.3 实践平台</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7"><span class="toc-number">9.2.</span> <span class="toc-text">8.2 开发工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-1-%E6%A1%86%E6%9E%B6%E5%92%8C%E5%BA%93"><span class="toc-number">9.2.1.</span> <span class="toc-text">8.2.1 框架和库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-2-%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7"><span class="toc-number">9.2.2.</span> <span class="toc-text">8.2.2 部署工具</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/06/30/service_manager/" title="service_manager"><img src="/img/service_manager.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="service_manager"/></a><div class="content"><a class="title" href="/2025/06/30/service_manager/" title="service_manager">service_manager</a><time datetime="2025-06-30T04:00:00.000Z" title="发表于 2025-06-30 12:00:00">2025-06-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/" title="机器学习之Logistic回归"><img src="/img/ljhg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之Logistic回归"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/" title="机器学习之Logistic回归">机器学习之Logistic回归</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="机器学习之集成学习"><img src="/img/jcxx.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之集成学习"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="机器学习之集成学习">机器学习之集成学习</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM/" title="机器学习之SVM"><img src="/img/svm.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之SVM"/></a><div class="content"><a class="title" href="/2025/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM/" title="机器学习之SVM">机器学习之SVM</a><time datetime="2025-06-11T01:00:00.000Z" title="发表于 2025-06-11 09:00:00">2025-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" title="机器学习之决策树详解：从原理到实践"><img src="/img/jcs.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习之决策树详解：从原理到实践"/></a><div class="content"><a class="title" href="/2025/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" title="机器学习之决策树详解：从原理到实践">机器学习之决策树详解：从原理到实践</a><time datetime="2025-06-10T01:00:00.000Z" title="发表于 2025-06-10 09:00:00">2025-06-10</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Dummy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.2.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>